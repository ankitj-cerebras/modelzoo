##############################################################
## Autoregressive LM Preprocessing Parameters
##############################################################

setup:
    input_dir: "/cb/datasets/language/pile/raw_data/train/"
    output_dir: "./data_dir_summarization/"
    processes: 1
    dataset_processor: "LMDataPreprocessor"

processing:
    tokenizer_type: "GPT2Tokenizer"
    vocab_file: "../../../vocab/gpt2-vocab.bpe"
    encoder_file: "../../../vocab/gpt2-encoder.json"

    max_seq_length: 2048
    short_seq_prob: 0.0

    output_name: "examples"
    files_per_record: 50000
    write_in_batch: True
    
    write_remainder: True
    resume_from_checkpoint: False
    display_pbar: True
    seed: 0

dataset:
    use_ftfy: True
    ftfy_normalizer: "NFC"
    wikitext_detokenize: False
    
    jsonl_key: "text"
    pack_sequences: True
