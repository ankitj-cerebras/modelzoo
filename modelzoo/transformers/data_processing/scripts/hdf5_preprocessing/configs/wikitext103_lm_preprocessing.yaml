##############################################################
## Autoregressive LM Preprocessing Parameters for wikitext103
## Prepare raw wikitext103 in .txt format
## TODO: Move formatted HDF5's to /cb/datasets/language/
##############################################################

setup:
    input_dir: "/cb/cold/andrewz/wikitext103/train"
    output_dir: "/cb/cold/andrewz/wikitext103-dataset-hdf5/train"
    processes: 1
    dataset_processor: "LMDataPreprocessor"

processing:
    tokenizer_type: "GPT2Tokenizer"
    vocab_file: "../../../vocab/gpt2-vocab.bpe"
    encoder_file: "../../../vocab/gpt2-encoder.json"

    max_seq_length: 2048
    short_seq_prob: 0.0

    output_name: "examples"
    files_per_record: 50000
    write_in_batch: True

    write_remainder: True
    resume_from_checkpoint: False
    display_pbar: True
    seed: 0

dataset:
    use_ftfy: True
    ftfy_normalizer: "NFC"
    wikitext_detokenize: True
    
    pack_sequences: True
