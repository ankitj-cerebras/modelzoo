# A 111M parameter Cerebras-GPT model config mu-transferred from the 40M base model.
# Optimal hyperparameters were determined in a hyperparameter sweep at 40M scale
# which were then mu-transferred to 111M model.

train_input:
  data_processor: GptHDF5MapDataProcessor
  data_dir:
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/0/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/1/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/2/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/3/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/4/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/5/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/6/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/7/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/8/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/9/
  shuffle: false # data is expected to be shuffled during preprocessing
  shuffle_seed: 0
  batch_size: 120
  num_workers: 8
  prefetch_factor: 10
  persistent_workers: true
eval_input:
  data_processor: GptHDF5DataProcessor
  data_dir: ./language/datasets/pile_original/hdf5_dataset/val_msl2048/
  batch_size: 120
  shuffle: false
  num_workers: 8
model:
  vocab_size: 50257
  hidden_size: 768
  use_position_embedding: true
  position_embedding_type: learned
  share_embedding_weights: true
  max_position_embeddings: 2048
  num_hidden_layers: 10
  dropout_rate: 0.0
  layer_norm_epsilon: 1.0e-05
  num_heads: 12
  attention_type: scaled_dot_product
  attention_dropout_rate: 0.0
  use_projection_bias_in_attention: true
  use_ffn_bias_in_attention: true
  filter_size: 3072
  nonlinearity: gelu
  use_ffn_bias: true
  embedding_initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.08
    a: -0.16
    b: 0.16
  initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.046188021535170064
    a: -0.09237604307034013
    b: 0.09237604307034013
  output_layer_initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.010327955589886445
    a: -0.02065591117977289
    b: 0.02065591117977289
  loss_scaling: batch_size
  loss_weight: 0.00048828125
  use_bias_in_output: false
  mixed_precision: true
  use_bfloat16: true
  output_logits_scale: 0.3333333333333333
  embeddings_scale: 10.0
  scale_qk_dot_by_d: true
optimizer:
  optimizer_type: AdamW
  betas: [0.9, 0.95]
  eps: 1.0e-08
  max_gradient_norm: 1.0
  learning_rate:
  - scheduler: Linear
    initial_learning_rate: 0.0
    end_learning_rate: 0.006
    total_iters: 1525
  - scheduler: Linear
    initial_learning_rate: 0.006
    end_learning_rate: 0.0006
    total_iters: 7512
  weight_decay: 0.1
  log_summaries: true
  correct_bias: true
  adjust_learning_rate:
    decoder_kernel: 0.3333333333333333
runconfig:
  max_steps: 9037
  eval_steps: 1546
  checkpoint_steps: 903
  enable_distributed: false
  log_steps: 1
  save_initial_checkpoint: false
  use_cs_grad_accum: true
  use_appliance_data: true
