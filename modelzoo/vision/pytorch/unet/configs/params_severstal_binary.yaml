# Params for UNet model on Severstal Binary labels dataset.

train_input: &train_input
    data_processor: SeverstalBinaryClassDataProcessor
    data_dir: "./computer_vision/datasets/severstal_kaggle"
    image_shape: [256, 256, 1] # [H, W, C]
    normalize_data_method: "zero_centered"
    augment_data: True
    shuffle: True
    train_test_split: 0.85
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 480
    num_classes: 2
    num_workers: 24
    class_id: 3
    prefetch_factor: 10
    persistent_workers: True
    use_fast_dataloader: True
    use_worker_cache: True

eval_input:
    <<: *train_input
    augment_data: False
    shuffle: False
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 64
    num_workers: 2

model:
    nonlinearity: "ReLU"
    skip_connect: True
    enable_bias: False
    downscale_method: "max_pool"
    convs_per_block: ["3x3_conv", "3x3_conv"]
    encoder_filters: [32, 64, 128, 256]
    decoder_filters: [128, 64, 32]
    residual_blocks: False
    initializer: 
        "name": "glorot_uniform"
        "gain": 1.0
    bias_initializer: "zeros"
    # bce -> Binary Cross Entropy With Logits
    loss: "bce"
    mixed_precision: True
    norm_layer: "batchnorm2d"
    use_bfloat16: True

optimizer:
    optimizer_type: "Adam"
    weight_decay: 0.0001
    learning_rate: 1.0e-4
    # Choices: `loss_scaling_factor`: {"dynamic", use values >= 1.0 for static loss scaling}
    # Note: When `use_bfloat16: True`, this always defaults to `1.0`
    loss_scaling_factor: 1.0

runconfig:
    max_steps: 10000
    log_steps: 100
    checkpoint_steps: 10000
    seed: 1
    save_initial_checkpoint: True
    num_csx: 1
    num_act_servers: 4
    num_wgt_servers: 1
    num_workers_per_csx: 16
    compile_crd_memory_gi: 90
    wrk_memory_gi: -1
