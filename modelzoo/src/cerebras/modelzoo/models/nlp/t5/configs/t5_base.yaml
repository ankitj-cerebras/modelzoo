trainer:
  init:
    backend:
      backend_type: CSX
    model_dir: ./model_dir
    seed: 1
    model:
      name: "t5"
      src_vocab_size: 32001
      extra_ids: 100
      ## Encoder
      encoder_num_hidden_layers: 12
      dropout_rate: 0.1
      # Encoder -- Attention
      d_kv: 64 # Size of the key, query, value projections per attention head.
      num_heads: 12 # d_kv * num_heads = hidden size.
      use_projection_bias_in_attention: false
      # Position Embeddings
      position_embedding_type: relative
      src_max_position_embeddings: 512
      tgt_max_position_embeddings: 114
      # Shared Weighed Embeddings
      share_embedding_weights: true
      share_encoder_decoder_embedding: true
      norm_type: layernorm # Disable T5 style layer norm (RMSNorm)
      # Encoder -- ffn
      d_ff: 3072 # Size of the intermediate feed forward layer in t5 blocks.
      d_model: 768 # Size of the encoder layers and the pooler layer.
      encoder_nonlinearity: relu # {"gelu", "relu", "geglu"}
      decoder_nonlinearity: relu # {"gelu", "relu", "geglu"}
      layer_norm_epsilon: 1.0e-05
      ## Decoder
      decoder_num_hidden_layers: 12
      # Loss scaling weight, 1/{average_number_valid_tokens}
      lm_loss_weight: 0.015
      # Loss scaling config: `precomputed_num_masked` (for vts only)
      #                      `batch_size` is scaling loss by batch size
      mlm_loss_scaling: precomputed_num_masked
      # Cerebras configs.
      mixed_precision: true
      fp16_type: cbfloat16
    optimizer:
      AdaFactor: {}
    schedulers:
    - InverseSquareRootDecayLR:
        warmup_steps: 10000
        scale: 1.0
    precision:
      # Cerebras configs.
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
    loop:
      max_steps: 524288
      eval_frequency: 10000
      eval_steps: 2510
    checkpoint:
      steps: 10000
    logging:
      log_steps: 100
  fit:
    train_dataloader:
      data_processor: T5DynamicDataProcessor
      src_vocab_file: ./t5/c4/vocab.txt # https//huggingface.co/t5-small/resolve/main/spiece.model.
      src_data_dir: ./t5/c4/en/train.tok.sentencepiece.3200
      extra_ids: 100 # A number of extra ids added to the end of the vocabulary for use as sentinels.
      src_max_sequence_length: 512
      tgt_max_sequence_length: 114
      shuffle: true
      shuffle_seed: 1
      shuffle_buffer: 16384 # large buffer size allows batches to contain samples from multiple documents
      # The effective batch size, which is evenly divided across "num_csx" systems used for the run
      batch_size: 216 # this gives roughly 2**16 tokens per batch
      # for GPU(16GB) set batch_size: 8
      # 8 * 27 = 216 with grad_accum_steps: 27
      num_workers: 8
      prefetch_factor: 10
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: T5DynamicDataProcessor
      src_vocab_file: ./t5/c4/vocab.txt
      src_data_dir: ./t5/c4/en/validation.tok.sentencepiece.3200
      extra_ids: 100
      src_max_sequence_length: 512
      tgt_max_sequence_length: 114
      shuffle: false
      shuffle_seed: 1
      batch_size: 216
      num_workers: 8
      prefetch_factor: 10
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
