# Data Processors Overview

There are seven different data processors in this directory. With all those choices,
you may be asking yourself "what are all these data processors for and which one should
I use?" This readme will help you answer that question for your particular use-case
and provide some useful tips and tricks for using both basic and advanced features
of our more commonly used data processors.

## Overview
To start with, here's a high level overview of what's here:

* [`GPTHDF5MapDataProcessor`](GptHDF5MapDataProcessor.py): This is a good default choice.
It has several nice features that make large language model training more convenient.
This data processor requires that you pre-process your data into HDF5 files. It is
highly recommend to shuffle your data during the pre-processing stage.

* [`GPTHDF5DataProcessor`](GptHDF5DataProcessor.py): This is another choice of a fast
and reliable data processor, but it lacks several of the nice features of the
`GPTHDF5MapDataProcessor`. When we have the choice, we like to use its map style
counterpart, but this data processor can be a useful fallback when you have unusually
slow disk performance and are unable to shuffle your data on disk (more on this below).

* [`HuggingFaceDataProcessorEli5`](HuggingFaceDataProcessorEli5.py) and [`HuggingFaceIterableDataProcessorEli5`](HuggingFaceIterableDataProcessorEli5.py): these
two data processors are useful if you're looking for a minimal effort way to port
an existing data pipeline that is based off of HuggingFace. For more information,
see [`src/models/data_preparation/huggingface/README.md`](../../../data_preparation/huggingface/README.md)

* [`DummyDataProcessor`](DummyDataProcessor.py) and [`DummyIterableDataProcessor`](DummyIterableDataProcessor.py): these two aren't practically
useful, but they give an example of how to convert generic PyTorch dataloaders
to work on CS system.


## GptHDF5MapDataProcessor

### Overview
Most of our example GPT-3 configs use this data processor, so those will
be good references for proper usage. This data processor has several features which make
large language model training more convenient, including:

* A data order that is completely independent of how many CS systems or workers you are using.
For example, you can train a model with `16` CS systems and have the exact same data order as if you trained
the same model with a single CS system and the same global batch size. In fact, the ordering
of the samples is independent of the batch size altogether, so if you want to change
batch size mid-run, you can safely do so without risking repeating samples (more on deterministic
restart later)

* \[corpus data format only\] Sequence length that is defined at run-time instead of preprocessing time. This means that
if you want to experiment with training runs of different sequence lengths, you don't need
to re-process the data and save a new version to disk every time you try a new setting.

* Mixing datasets at runtime according to user-supplied weights.

* Robust deterministic restart that allows you to continue a run with a
different number of CS systems that you used to start the run.

* \[advanced\] The ability to consider only a subset of a dataset for a particular training
run. This can be useful for things like sequence length scheduling or small-scale testing of
multi-epoch behavior.

This data processor supports two different formats of data on disk:

* The "sample format", which is what is used for the `GPTHDF5DataProcessor`, and can be written
using the tools in [`modelzoo/data_preparation/data_preprocessing/`](../../../data_preparation/data_preprocessing)
and shuffled using the tools in
[`modelzoo/data_preparation/hdf5_shuffling`](../../../data_preparation/hdf5_shuffling).

* The "corpus format", which is unique to this data processor and can be written using
the tools in [`modelzoo/data/common/h5_map_dataset/preprocess_pile.py`](../../common/h5_map_dataset//preprocess_pile.py).

Currently we suggest using the sample format as a good default choice as it allows
you to shuffle your data during preprocessing, significantly increasing the speed of
the data processor at runtime, which can be an issue if you have an unusually slow storage system.


### Preprocessing
**Sample format:** There are two steps required to preprocess data into the sample format
for use in the `GptHDF5MapDataProcessor`:

* Tokenize the data into hdf5 files using the tools in
[`modelzoo/data_preparation/data_preprocessing`](../../../data_preparation/data_preprocessing)

* Shuffle the data in the hdf5 files generated by the previous step using the tools in
[`modelzoo/data_preparation/hdf5_shuffling`](../../../data_preparation/hdf5_shuffling)

Although not strictly necessary, the shuffling is an important preprocessing step
as it allows you to not shuffle at runtime, which increases the throughput
of the data processor. If you leave shuffling for runtime, this can cause performance
issues if your storage is unusually slow.

**Corpus format:** To preprocess data into the corpus format, you can refer to
[`modelzoo/data/common/h5_map_dataset/preprocess_pile.py`](../../common/h5_map_dataset/preprocess_pile.py).
This script preprocesses the Pile dataset into the appropriate format. Regardless
of the source of your corpus, the process will be essentially the same, although
it is possible that you will have to modify the reading logic in the script to
support custom datasets.


### Basic usage

Here's an example configuration setting for the corpus format

```yaml
train_input:
  data_processor: GPTHDF5MapDataProcessor
  data_dir: /path/to/data
  max_sequence_length: 2048
  batch_size: 1024
  shuffle: True
  num_workers: 8
```

and a similar configuration for the sample format

```yaml
train_input:
  data_processor: GPTHDF5MapDataProcessor
  data_dir: /path/to/data
  batch_size: 1024
  shuffle: False # better to shuffle during preprocessing
  num_workers: 8
```

### Which data format should I choose?
If you are finetuning or have some other application where you need to mask out part of
the input during loss computation, then you need to use the sample format. The corpus
format is only useful for traditional GPT style pretraining with packed sequences
drawn from a contiguous corpus of text. This does cover a large fraction of the potential
use-cases though.

If you intend to experiment with different sequence lengths or even sequence length
scheduling, the corpus format is probably right for you as it easily enables this flexibility.

If your storage is unusually slow, the corpus format might not be right for you. This format
requires shuffling at runtime, which is accomplished by random access reads to disk. Although
typical storage setups are plenty performant for this, some slower storage systems might
not be able to keep up with this demand. One remedy is to set `train_input.use_worker_cache: True`,
which tells the data processor to copy the data to a staging area on each worker node
before the start of the run. Another solution is to simply use the sample format. With the
sample format you can shuffle at preprocessing time, which enables sequential disk reads
at runtime, increasing the performance of the data processor.

Beyond these considerations, you are free to use whichever format you want. We tend
to prefer the sample format so that we can do our shuffling during preprocessing,
but either format will work well for most pretraining workloads.


### Dataset mixtures
It's fairly common to want to train with data from multiple different domains. For example,
lets suppose that you want to train a model on both English and Spanish data, but you're
not sure how much of your data should be in English vs Spanish to get your desired end
properties from your model. In this case, you can put the following in your configuration

```yaml
train_input:
  data_processor: GPTHDF5MapDataProcessor
  mixture:
    - data_dir: /path/to/english/data
      weight: 0.6
    - data_dir: /path/to/spanish/data
      weight: 0.4
  max_sequence_length: 2048
  batch_size: 1024
  shuffle: True
  num_workers: 8
```

Samples are drawn at runtime according to the weights provided (normalized to sum to 1),
and the entire process is still completely deterministic, agnostic of your cluster setup,
and amenable to deterministic restart. This allows you to easily play around with the
relative weights without inconvenient preprocessing steps. This interface extends
to mixing together arbitrarily many datasets.


### Deterministic restart

The main feature that can be quite useful for large training runs but isn't self
evident from the example configurations is the deterministic restart feature. If you
are training a multi-billion parameter model over the course of days or even weeks, it
is useful to have the ability to pause a run in the middle and then restart again as if
nothing ever happened. Part of what is required for this is having a dataloader that
can pick up where it left off, maintaining the same data order as before and avoiding
any unwanted repetition in the data seen by the model. This behavior is supported
both for the `GptHDF5DataProcessor` and the `GptHDF5MapDataProcessor`, and we call it
"deterministic restart".

Deterministic restart is automatically enabled in the `GptHDF5DataProcessor` when running with this repo's run scripts. As long
as you don't change the shuffle seed, you can safely pause and restart your
training runs with the guarantee that the dataloader will pick back up as if there
was never any interruption. This holds true even if you are shuffling at runtime
or you change the configuration of the cluster you are running on (e.g. you go from
using `8` CS systems to `16` CS systems).

Internally, this works since this data processor conforms to the `RestartableDataloader` protocol. By calling
`dataloader.state_dict()` at each checkpoint step, the dataloader state is returned
and can be saved into the checkpoint, similar to how model weights are stored.
Cerebras ModelZoo training script automatically does this by default for such
dataloaders. When you resume model training from a checkpoint, a call is made to
`dataloader.load_state_dict(state)`, where `state` is what's loaded from the
previously saved checkpoint file. The dataloader then picks up that state
and resumes from that point. Again, Cerebras ModelZoo training scripts automatically
do this for restartable dataloader and when a dataloader state is found in the
checkpoint, but if you write your own data processor or training loop, this use
case can serve as a good reference.

There are some situations in which you might want to restart the dataloader from
scratch as opposed to from a previous checkpoint. Using the modelzoo run scripts,
this can be done using the `--load_checkpoint_states` flag, for example
`--load_checkpoint_states="model,optimizer,global_step,lr_scheduler"` will load
everything except for the dataloader state, and `--load_checkpoint_states="model"`
will only load the model state and reset everything else.


### Advanced features
Most users won't need these features, and for the common use-case they will cause more
confusion than good. That said, in the right scenario they are powerful tools for
advanced users.

**Shuffling across epochs:** If you are training for several epochs, it can be beneficial
to sample uniformly from a pool of data repeated `num_epochs` times rather than to
sequentially repeat the same data one epoch after the next. If you are training for two epochs,
then this means that some samples will be seen twice before others are seen at all, but at
the end of the two epochs every sample will have been seen exactly twice. This technique is used in
the data processors of several of the widely used open source LLM repos. In order to enable
this type of shuffling in the `GptHDF5MapDataProcessor`, set `train_input.num_samples` to
the expected total number of training samples across all epochs of training.

**Data subsetting:** This feature allows you to consider
only a fraction of the dataset at hand. This can be useful for some advanced experimentation
such as training with different sequence lengths for different stages of a run or
artificially limiting the amount of training data available. To set this, simply add
something like `train_input.data_subset: 0.0-0.5` to your configuration. This setting will
expose only the first half of the dataset to the model and ignore the second half.
The syntax also supports combinations of ranges like `0.1-0.3,0.8-1.0`, and the corresponding
flag in the case of a mixture dataset is



## GptHDF5DataProcessor
This data processor is more restricted compared to the one discussed above. Its shuffle
is a buffered shuffle rather than a global shuffle, and it doesn't support dataset
mixtures, data subsetting, or multi-epoch shuffling. Deterministic restart still
works for this data processor, but only if the total number of workers per CS system
remains constant across different stages of the run and shuffling is disabled.

The primary reason to use this data processor is if you need the buffered shuffling
functionality because of some combination of being unable to shuffle at preprocessing
time and having a disk that is too slow to support the true shuffle that is performed
by the `GptHDF5MapDataProcessor`.

In order to pre-process data for use with this data processor, follow the same
instructions given for preprocessing the sample format of the map style data processor.
