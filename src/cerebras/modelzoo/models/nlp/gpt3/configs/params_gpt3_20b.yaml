train_input:
    data_processor: "GptHDF5MapDataProcessor"
    data_dir:
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/0/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/1/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/2/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/3/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/4/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/5/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/6/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/7/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/8/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/9/
    shuffle: False # data is expected to be shuffled during preprocessing
    shuffle_seed: 256
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 77
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

eval_input:
    data_processor: "GptHDF5MapDataProcessor"
    data_dir: "./language/datasets/pile_original/hdf5_dataset/val_msl2048/"
    shuffle: False
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 77
    num_workers: 8

### Model
model:
    # Embedding
    hidden_size: 6144
    vocab_size: 50257
    position_embedding_type: "learned"
    share_embedding_weights: True
    max_position_embeddings: 2048

    # Encoder
    num_hidden_layers: 44
    dropout_rate: 0.0
    layer_norm_epsilon: 1.0e-5 # change to 1.0e-12 for single precision training

    # Encoder - Attention
    num_heads: 64
    attention_type: "scaled_dot_product"
    attention_dropout_rate: 0.0
    use_projection_bias_in_attention: True
    use_ffn_bias_in_attention: True

    embedding_initializer:
       name: "truncated_normal"
       mean: 0.0
       std: 0.02

    initializer:
       name: "truncated_normal"
       mean: 0.0
       std: 0.02

    output_layer_initializer:
      name: "truncated_normal"
      mean: 0.0
      std: 0.0021320071635561044 # 0.02 / sqrt(2 * num_hidden_layers)

    # Encoder - ffn
    filter_size: 24576
    nonlinearity: "gelu"
    use_ffn_bias: True

    # Task-specific
    use_bias_in_output: False
    loss_scaling: "num_tokens"
    loss_weight: 1.0

    # Cerebras parameters
    mixed_precision: True
    boundary_casting: False
    fp16_type: "cbfloat16"

### Optimization
optimizer:
    optimizer_type: "adamw"
    betas: [0.9, 0.95]
    eps: 1.0e-8
    correct_bias: True
    weight_decay: 0.01
    max_gradient_norm: 1.0
    # GPT-3 warms-up training over the first 375M tokens
    # Then, the DeepMind approach cosine decays learning rate over the rest
    # of the training steps
    learning_rate:
     - scheduler: "Linear"
       initial_learning_rate: 0.0
       end_learning_rate: 5.5e-5
       total_iters: 649
     - scheduler: "CosineDecay"
       initial_learning_rate: 5.5e-5
       end_learning_rate: 5.5e-6
       total_iters: 170129
    loss_scaling_factor: "dynamic"
    initial_loss_scale: 2147483648.0
    max_loss_scale: 2147483648.0
    log_summaries: True
    ws_summary: True

### Cerebras parameters
runconfig:
  max_steps: 170778
  eval_steps: 2409
  checkpoint_steps: 5000
  max_checkpoints: 21
  log_steps: 1
  save_initial_checkpoint: True
  seed: 0
  enable_distributed: False # Change to True on GPU
  num_workers_per_csx: 1
  num_wgt_servers: 24
