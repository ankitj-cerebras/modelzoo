# A 256M parameter Cerebras-GPT model config mu-transferred from the 40M parameter
# base model.
# Optimal hyperparameters were determined in a hyperparameter sweep at 40M scale
# which were then mu-transferred to 256M parameter model.

train_input:
  data_processor: GptHDF5MapDataProcessor
  data_dir:
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/0/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/1/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/2/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/3/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/4/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/5/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/6/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/7/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/8/
  - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/9/
  shuffle: false # data is expected to be shuffled during preprocessing
  shuffle_seed: 0
  batch_size: 264
  num_workers: 8
  prefetch_factor: 10
  persistent_workers: true
eval_input:
  data_processor: GptHDF5DataProcessor
  data_dir: ./language/datasets/pile_original/hdf5_dataset/val_msl2048/
  batch_size: 264
  shuffle: false
  num_workers: 8
model:
  vocab_size: 50257
  hidden_size: 1088
  position_embedding_type: learned
  share_embedding_weights: true
  max_position_embeddings: 2048
  num_hidden_layers: 14
  dropout_rate: 0.0
  layer_norm_epsilon: 1.0e-05
  num_heads: 17
  attention_type: scaled_dot_product
  attention_dropout_rate: 0.0
  use_projection_bias_in_attention: true
  use_ffn_bias_in_attention: true
  filter_size: 4352
  nonlinearity: gelu
  use_ffn_bias: true
  embedding_initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.08
    a: -0.16
    b: 0.16
  initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.03880570000581328
    a: -0.07761140001162656
    b: 0.07761140001162656
  output_layer_initializer:
    mean: 0.0
    name: truncated_normal
    std: 0.007333587976225691
    a: -0.014667175952451383
    b: 0.014667175952451383
  loss_scaling: num_tokens
  loss_weight: 1.0
  use_bias_in_output: false
  mixed_precision: true
  fp16_type: "cbfloat16"
  output_logits_scale: 0.23529411764705882
  embeddings_scale: 10.0
  scale_qk_dot_by_d: true
optimizer:
  optimizer_type: AdamW
  betas: [0.9, 0.95]
  eps: 1.0e-08
  max_gradient_norm: 1.0
  learning_rate:
  - scheduler: Linear
    initial_learning_rate: 0.0
    end_learning_rate: 0.006
    total_iters: 693
  - scheduler: Linear
    initial_learning_rate: 0.006
    end_learning_rate: 0.0006
    total_iters: 8775
  weight_decay: 0.1
  log_summaries: true
  correct_bias: true
  loss_scaling_factor: "dynamic"
  adjust_learning_rate:
    decoder_kernel: 0.23529411764705882
runconfig:
  max_steps: 9468
  eval_steps: 702
  checkpoint_steps: 946
  enable_distributed: false
  log_steps: 1
  save_initial_checkpoint: false
