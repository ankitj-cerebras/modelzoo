train_input:
    batch_size: 3200
    data_dir: ./datasets/multilingual_v2/finetuning_v5_all_train
    data_processor: GptHDF5MapDataProcessor
    micro_batch_size: auto
    num_workers: 1
    persistent_workers: true
    prefetch_factor: 10
    repeat: true
    shuffle: false
    shuffle_seed: 1
    use_worker_cache: false
    vocab_size: 84992
eval_input:
    batch_size: 32
    data_dir: ./datasets/multilingual_v2/finetuning_v5_all_valid
    data_processor: GptHDF5MapDataProcessor
    micro_batch_size: auto
    num_workers: 1
    repeat: false
    shuffle: false
    use_worker_cache: false
    vocab_size: 84992
model:
    attention_dropout_rate: 0.0
    attention_kernel: optimized_beta
    attention_softmax_fp32: true
    attention_type: scaled_dot_product
    boundary_casting: false
    dropout_rate: 0.0
    embedding_initializer:
        a: -0.146
        b: 0.146
        mean: 0.0
        name: truncated_normal
        std: 0.073
    embeddings_scale: 14.6
    filter_size: 13653
    fp16_type: bfloat16
    hidden_size: 5120
    initializer:
        a: -0.03264659247149693
        b: 0.03264659247149693
        mean: 0.0
        name: truncated_normal
        std: 0.016323296235748463
    layer_norm_epsilon: 1.0e-05
    loss_scaling: batch_size
    loss_weight: 0.020268227843388255
    max_position_embeddings: 2048
    mixed_precision: true
    nonlinearity: swiglu
    norm_type: layernorm
    num_heads: 40
    num_hidden_layers: 40
    output_layer_initializer:
        a: -0.0036499999999999996
        b: 0.0036499999999999996
        mean: 0.0
        name: truncated_normal
        std: 0.0018249999999999998
    output_logits_scale: 0.11100000000000002
    position_embedding_type: alibi
    scale_qk_dot_by_d: true
    share_embedding_weights: true
    use_bias_in_output: false
    use_ffn_bias: true
    use_ffn_bias_in_attention: true
    use_projection_bias_in_attention: true
    vocab_size: 84992
optimizer:
    adjust_learning_rate:
        decoder_kernel: 0.05
    betas:
    - 0.9
    - 0.95
    correct_bias: true
    eps: 1.0e-09
    learning_rate:
    -   end_learning_rate: 0.00067
        initial_learning_rate: 0.0
        scheduler: Linear
        total_iters: 400
    -   end_learning_rate: 6.7e-05
        initial_learning_rate: 0.00067
        scheduler: Linear
        total_iters: 6298
    log_summaries: true
    loss_scaling_factor: 1.0
    max_gradient_norm: 1.0
    optimizer_type: AdamW
    weight_decay: 0.1
runconfig:
    checkpoint_steps: 335
    eval_steps: 2834
    log_steps: 1
    max_steps: 6698
    precision_opt_level: 1
    save_initial_checkpoint: false
    seed: 1
    transfer_processes: 15
