train_input:
    batch_size: 2640
    data_processor: GptHDF5MapDataProcessor
    mixture:
    -   data_dir: ./datasets/multilingual_v2/pile_train_correct
        weight: 0.6510508098635567
    -   data_dir: ./datasets/multilingual_v2/github_train
        weight: 0.05508760163741673
    -   data_dir: ./datasets/multilingual_v2/books_3_arabic_train_correct_packed
        weight: 0.031560734257525036
    -   data_dir: ./datasets/multilingual_v2/AraV5/alkhair_train_packed
        weight: 0.0008565650109973057
    -   data_dir: ./datasets/multilingual_v2/AraV5/aranews_train_packed
        weight: 0.00015934634990853936
    -   data_dir: ./datasets/multilingual_v2/AraV5/baai_train_packed
        weight: 0.026914907066381152
    -   data_dir: ./datasets/multilingual_v2/AraV5/C4_train_packed
        weight: 0.04434604029033599
    -   data_dir: ./datasets/multilingual_v2/AraV5/ccnews_train_packed
        weight: 0.006921611120055068
    -   data_dir: ./datasets/multilingual_v2/AraV5/common_crawl_train_packed
        weight: 0.1665541286308144
    -   data_dir: ./datasets/multilingual_v2/AraV5/daypop_train_packed
        weight: 0.0017987284534324059
    -   data_dir: ./datasets/multilingual_v2/AraV5/en2ar_wikipedia_train_packed
        weight: 0.006428621520916274
    -   data_dir: ./datasets/multilingual_v2/AraV5/en_wikipedia_train_packed
        weight: 0.003561363619881376
    -   data_dir: ./datasets/multilingual_v2/AraV5/maktabah_train_packed
        weight: 0.0026810119295565364
    -   data_dir: ./datasets/multilingual_v2/AraV5/misc_train_packed
        weight: 7.056663329594336e-05
    -   data_dir: ./datasets/multilingual_v2/AraV5/osian_train_packed
        weight: 0.0006335432478047283
    -   data_dir: ./datasets/multilingual_v2/AraV5/un_train_packed
        weight: 0.0010203462676059763
    -   data_dir: ./datasets/multilingual_v2/AraV5/wikipedia_train_packed
        weight: 0.00035407410051588393
    num_workers: 1
    persistent_workers: true
    prefetch_factor: 10
    shuffle: false
    shuffle_seed: 1
    use_worker_cache: false
    vocab_size: 84992
eval_input:
    batch_size: 32
    data_dir: ./datasets/multilingual_v2/pile_val_correct_packed
    data_processor: GptHDF5MapDataProcessor
    num_workers: 1
    repeat: false
    shuffle: false
    use_worker_cache: false
    vocab_size: 84992
model:
    attention_dropout_rate: 0.0
    attention_kernel: optimized_beta
    attention_type: scaled_dot_product
    boundary_casting: false
    dropout_rate: 0.0
    embedding_initializer:
        a: -0.146
        b: 0.146
        mean: 0.0
        name: truncated_normal
        std: 0.073
    embeddings_scale: 14.6
    filter_size: 19114
    fp16_type: cbfloat16
    hidden_size: 7168
    initializer:
        a: -0.027591406529673585
        b: 0.027591406529673585
        mean: 0.0
        name: truncated_normal
        std: 0.013795703264836793
    layer_norm_epsilon: 1.0e-05
    loss_scaling: batch_size
    loss_weight: 0.00048828125
    max_position_embeddings: 2048
    mixed_precision: true
    nonlinearity: swiglu
    num_heads: 56
    num_hidden_layers: 48
    output_layer_initializer:
        a: -0.0028160361368081773
        b: 0.0028160361368081773
        mean: 0.0
        name: truncated_normal
        std: 0.0014080180684040886
    output_logits_scale: 0.07928571428571429
    position_embedding_type: alibi
    scale_qk_dot_by_d: true
    share_embedding_weights: true
    use_bias_in_output: false
    use_ffn_bias: true
    use_ffn_bias_in_attention: true
    use_projection_bias_in_attention: true
    vocab_size: 84992
optimizer:
    adjust_learning_rate:
        decoder_kernel: 0.03571428571428571
    betas:
    - 0.9
    - 0.95
    correct_bias: true
    eps: 8.0e-10
    learning_rate:
    -   end_learning_rate: 0.012
        initial_learning_rate: 0.0
        scheduler: Linear
        total_iters: 69
    -   end_learning_rate: 0.0012
        initial_learning_rate: 0.012
        scheduler: Linear
        total_iters: 106522
    log_summaries: true
    max_gradient_norm: 1.0
    optimizer_type: AdamW
    weight_decay: 0.1
    loss_scaling_factor: dynamic
runconfig:
    checkpoint_steps: 10660
    log_steps: 1
    max_steps: 106591
    precision_opt_level: 1
    save_initial_checkpoint: false
    seed: 1
