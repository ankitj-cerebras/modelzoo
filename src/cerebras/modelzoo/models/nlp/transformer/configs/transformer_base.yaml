# Transformer Base - according to Table 3.
# Based on transformer_base_v1() :
# https://github.com/tensorflow/tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/tensor2tensor/models/transformer.py#L1771

train_input:
    data_processor : "TransformerDynamicDataProcessor"
    src_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.de"
    src_data_dir: "./transformer/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.en"
    tgt_data_dir: "./transformer/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 2048 # for GPU(16GB) set batch_size: 16
                     # 16 * 256 = 4096 with grad_accum_steps: 256
    shuffle: True
    shuffle_seed: 1
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

eval_input:
    data_processor : "TransformerDynamicDataProcessor"
    src_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.de"
    src_data_dir: "./transformer/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.en"
    tgt_data_dir: "./transformer/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    shuffle: False
    shuffle_seed: 1 # also for deterministic masking
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 4 # avoid dropping eval samples for CS runs
    # suggested batching settings for GPU (16GB) runs:
    # batch_size: 64
    # drop_last: False
    num_workers: 4
    prefetch_factor: 10
    persistent_workers: True

model:
    src_vocab_size: 36550
    tgt_vocab_size: 36550

    ## Encoder
    encoder_num_hidden_layers: 6
    dropout_rate: 0.1
    relu_dropout_rate: 0.0
    use_pre_encoder_decoder_dropout: True
    use_dropout_outside_residual_path: False

    # Encoder -- Attention
    d_kv: 64 # Size of the key, query, value projections per attention head.
    num_heads: 8 # d_kv * num_heads = hidden size(i.e. d_model)
    use_projection_bias_in_attention: False

    # Position Embeddings
    position_embedding_type: "fixed"

    # Shared Weighed Embeddings
    share_embedding_weights: True
    share_encoder_decoder_embedding: True

    norm_type: "layernorm" # Disable T5 style layer norm (RMSNorm)
    use_pre_encoder_decoder_layer_norm: False

    # Encoder -- ffn
    d_ff: 2048 # Size of the intermediate feed forward layer in t5 blocks.
    d_model: 512  # Size of the encoder layers and the pooler layer.
    encoder_nonlinearity: "relu" # {"gelu", "relu", "geglu"}
    decoder_nonlinearity: "relu" # {"gelu", "relu", "geglu"}
    layer_norm_epsilon: 1.0e-5
    use_ffn_bias: True

    ## Decoder
    decoder_num_hidden_layers: 6

    # Loss scaling weight, 1/{average_number_valid_tokens}
    lm_loss_weight: 0.033

    use_transformer_initialization: True

    # Loss scaling config: `precomputed_num_masked` (for vts only)
    #                      `batch_size` is scaling loss by batch size
    mlm_loss_scaling: "precomputed_num_masked"
    # Cerebras configs.
    mixed_precision: True


optimizer:
    optimizer_type: "Adam"
    betas: [0.9, 0.98]
    eps: 1.0e-6
    learning_rate:
        - scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.000698683912937353  # (d_model(=512)**-0.5) * (warm=4000**-0.5)
          total_iters: 4000
        - scheduler: "Linear"
          initial_learning_rate: 0.000698683912937353
          end_learning_rate: 0.0004710847104863831
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.0004710847104863831
          end_learning_rate: 0.00037894798246424215
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00037894798246424215
          end_learning_rate: 0.0003257949189616652
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.0003257949189616652
          end_learning_rate: 0.00029014271289331547
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00029014271289331547
          end_learning_rate: 0.000264105988466404
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.000264105988466404
          end_learning_rate: 0.00024401778319601815
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00024401778319601815
          end_learning_rate: 0.00022791101850398209
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00022791101850398209
          end_learning_rate: 0.00021462335024901533
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00021462335024901533
          end_learning_rate: 0.00020341801856705902
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00020341801856705902
          end_learning_rate: 0.00019380240931989222
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00019380240931989222
          end_learning_rate: 0.00018543300176721207
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00018543300176721207
          end_learning_rate: 0.00017806195541597027
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00017806195541597027
          end_learning_rate: 0.00017150536478269344
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00017150536478269344
          end_learning_rate: 0.00016562350566866137
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00016562350566866137
          end_learning_rate: 0.0001603080628584663
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.0001603080628584663
          end_learning_rate: 0.00015547359888418554
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00015547359888418554
          end_learning_rate: 0.00015105169410466443
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00015105169410466443
          end_learning_rate: 0.00014698682270697125
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00014698682270697125
          end_learning_rate: 0.00014323338788643664
          total_iters: 4800
        - scheduler: "Linear"
          initial_learning_rate: 0.00014323338788643664
          end_learning_rate: 0.00013975354982773464
          total_iters: 4800
    loss_scaling_factor: "dynamic"
#    grad_accum_steps: 256 # helps fit in GPU memory

runconfig:
    max_steps: 100000
    log_steps: 100
    checkpoint_steps: 10000
    seed: 1
    model_dir: "./model_dir"
    eval_steps: 3001 # comment out this line for GPU eval
